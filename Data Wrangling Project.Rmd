---
title: "Capstone Data Wrangling"
author: "Simone Zanetti"
date: "10/5/2018"
output: html_document
---

By the end of this phase, the two vital dataset for the next phases will be the following:

**1) data_exploratory**

**2) aggregate_data_last**


```{r}
library(tidyr)
library(dplyr)
library(chron)
library(ggplot2)
library(ggmap)
```

```{r}
data_clean <- 
```
              
# ERASE THE FIRST ROW THAT IS NOT RELEVANT TO MY DATASET
```{r}
data_clean <- data_clean[-1,]
```

----------------------------------------

# 1. CHANGE NAME OF EACH VARIABLE TO MAKE THEM UNDERSTABLE
```{r}
data_clean <- data_clean %>% rename( "dep_to_bs" = `DATA PART`,
                                     "arr_to_bs" = `DATA ARR`,
                                     "delivery_day" = `DATA CONS`,
                                     "sender_who" = MITT,
                                     "addresser_who" = DITTA,
                                     "address_delivery"= IND,
                                     "district_delivery" = LOC,
                                     "weight_pack" = PESO,        
                                     "num_pack"= COLLI,
                                     "driver_code" = CODICE,
                                     "pickup_who"= FIRMA,
                                     "pickup_time" = ORA)
```

## Uniform each name to Lower character

```{r}
data_clean <- data_clean %>% 
              mutate_all(funs(tolower(.)))
```


----------------------------------------

# 2. DEALING WITH MISSING VALUES

I provide an *overwiew of which variables contain missing values* and *how many* creating a temporary variable useful for the analysis.
```{r}
missing_values <- data_clean %>% summarise_all(funs(sum(is.na(.)))) %>% gather(variable, num_NA) 
```

Once I identified which variables contain missing values I decide how to manage them:

**1. addressee_who**
```{r}
data_clean %>% filter(is.na(addresser_who))      #in order to check which rows contain NA in the variable and decide how to manage it
data_clean <- data_clean %>%  mutate(addresser_who = replace(addresser_who,is.na(addresser_who), "UNKNOWN"))
```

**2. address_delivery**
```{r}
data_clean %>% filter(is.na(address_delivery))   #in order to check which rows contain NA in the variable and decide how to manage it
data_clean <- data_clean %>%  
              mutate(address_delivery = replace(address_delivery,addresser_who == "giancarlo pagnoni", "villaggio badia,trav. seconda")) # I could see that "giancarlo pagnoni" has lot of deliveries: in this way I have modified the NA using the address of the deliveries with no NA.
data_clean <- data_clean %>%  mutate(address_delivery = replace(address_delivery,is.na(address_delivery), "UNKNOWN"))
```


**3. driver_code**
```{r}
data_clean %>% filter(is.na(driver_code))    #in order to check which rows contain NA in the variable and decide how to manage it
```
I do not need the rows with "fermo deposito" as address delivery, as it means these deliveries have been picked up directly on the factory
```{r}
data_clean <- data_clean %>% filter(data_clean$address_delivery != "fermo deposito")
```

**4. pickup_time**

I cannot recover the missing hours, but I can lead an analysis of the missing hours to eventually find out trends.
So, I create a dataframe with the missing hours
```{r}
NA_pickuptime_foranalysis <- data_clean %>%  mutate(pickup_time = replace(pickup_time,is.na(pickup_time), 0 )) %>% 
                 filter(pickup_time == 0)
```

After, I delete the rows containing the Missing hours.
```{r}
data_clean<- data_clean %>% filter(data_clean$pickup_time > 0)
```

----------------------------------------

# 3. FIXING DIFFERENCES OF SPELLING (on variables in which this is necessary)

**A.DISTRICT_DELIVERY**  
I create a variable to analyse 
```{r}
district_distinct <- group_by(data_clean, district_delivery) %>% summarise()  
```

```{r}
data_clean <- data_clean %>% 
  mutate(district_delivery = sub(pattern = "roe.*" , "roe' volciano", district_delivery))
```

**B.ADDRESS_DELIVERY**  
I create a variable to analyse 
```{r}
address_distinct <- group_by(data_clean, address_delivery) %>% summarise()  
```
In this case I have over 80k rows.
*First thing* I should do is to erase the number on the street that make the research too unusefully precise.
```{r}
data_clean <- data_clean %>%
              mutate(address_delivery= gsub("[[:digit:]]","",address_delivery))  
              # QUESTION N.3: MAYBE AVOID ALL THE PUNCTUATION WITH [:punct:] would work ?
```

----------------------------------------

# 4. TURN ADDRESS INTO COORDINATES 

In this particular case I have Two possibilities of approach:

1) I **associate each district with its CAP** (the italian zip code) in order to perform my analysis within each CAP.  

In order to do so, I add to each row of "district_distinct" the name "Brescia" that will be useful for the Google geocoding system to recover the necessary data regarding each district.

```{r}
district_distinct <- district_distinct %>%  mutate(city = "Brescia") %>% unite(district_distinct, district_delivery, city, sep = "-" ) # IS THERE A SHORTER WAY ?

district_distinct$district_distinct <- as.character(district_distinct$district_distinct)

googlegeo <- geocode(location = district_distinct$district_distinct, output = c("more"))

googlegeo <- googlegeo %>% select( locality, administrative_area_level_3,postal_code)

googlegeo <- bind_cols(googlegeo, district_distinct)

googlegeoNA <- filter(googlegeo, is.na(googlegeo$postal_code))

googlegeoNA <- googlegeoNA %>%  select(district_distinct)

googlegeoNA$district_distinct <- as.character(googlegeoNA$district_distinct)

# -------- Repeat the operation to fill the NA

googlegeo2 <- geocode(location = googlegeoNA$district_distinct, output = c("more"))

googlegeo2 <- googlegeo2 %>% select( locality, administrative_area_level_3,postal_code)

googlegeo2 <- bind_cols(googlegeo2, googlegeoNA)

googlegeo2NA <- filter(googlegeo2, is.na(googlegeo2$postal_code)) 

googlegeo2NA <- googlegeo2NA %>%  select(district_distinct)

googlegeo2NA$district_distinct <- as.character(googlegeo2NA$district_distinct)

# -------- Repeat the operation to fill the NA

googlegeo3 <- geocode(location = googlegeo2NA$district_distinct, output = c("more")) 

googlegeo3 <- googlegeo3 %>% select( locality, administrative_area_level_3,postal_code)

googlegeo3 <- bind_cols(googlegeo3, googlegeo2NA)

googlegeo3NA <- filter(googlegeo3, is.na(googlegeo3$postal_code)) 

googlegeo3NA <- googlegeo3NA %>%  select(district_distinct)

googlegeo3NA$district_distinct <- as.character(googlegeo3NA$district_distinct)

# -------- Repeat the operation to fill the NA

googlegeo4 <- geocode(location = googlegeo3NA$district_distinct, output = c("more")) 

googlegeo4 <- googlegeo4 %>% select( locality, administrative_area_level_3,postal_code)

googlegeo4 <- bind_cols(googlegeo4, googlegeo3NA)

# OPERATION NECESSARY TO OBTAIN A COMPLETE DATAFRAME

googlegeo1_clean <- googlegeo %>% filter(!is.na(googlegeo$postal_code))

googlegeo2_clean <- googlegeo2 %>% filter(!is.na(googlegeo2$postal_code))

googlegeo3_clean <- googlegeo3 %>% filter(!is.na(googlegeo3$postal_code))

finalcapgeocoding <- bind_rows(googlegeo1_clean,googlegeo2_clean,googlegeo3_clean,googlegeo4)

finalcapgeocoding <- finalcapgeocoding %>% mutate(district_distinct = gsub(pattern = "-Brescia$", replacement = "",district_distinct))

finalcapgeocoding <- finalcapgeocoding %>% rename( "district_delivery" = district_distinct )

data_clean <- left_join(data_clean,finalcapgeocoding, by = "district_delivery")


```

I have to **analyse the NA of these new variables**

### Postal_codeNA

```{r}
postal_codeNA <- unique(postal_codeNA$district_delivery)
View(postal_codeNA)
```

I manually fill in the missing values

```{r}

data_clean <- data_clean %>% 
  mutate(postal_code = replace(postal_code, district_delivery == "ghedi", "25016") , 
           postal_code = replace(postal_code, district_delivery == "san faustino", "25070") , 
           postal_code = replace(postal_code, district_delivery == "agnosine", "25071"),
           postal_code = replace(postal_code, district_delivery == "tavernole sul mella", "25060"),
         postal_code = replace(postal_code, district_delivery== "pezzo", "25056"),
         postal_code = replace(postal_code, district_delivery== "corzano", "25030"),
         postal_code = replace(postal_code, district_delivery== "san felice del benaco", "25010"),
         postal_code = replace(postal_code, district_delivery== "fornaci", "25131"),
         postal_code = replace(postal_code, district_delivery== "gardone riviera", "25083"),
         postal_code = replace(postal_code, district_delivery== "pregno", "25069"),
         postal_code = replace(postal_code, district_delivery== "gazzane", "25070"),
         postal_code = replace(postal_code, district_delivery== "toscolano m.", "25088"),
         postal_code = replace(postal_code, district_delivery== "lugana", "25019"),
         postal_code = replace(postal_code, district_delivery== "mandolossa", "25030"),
         postal_code = replace(postal_code, district_delivery== "brescia", "25121/25136"),
         postal_code = replace(postal_code, district_delivery== "lido di lonato", "25017"),
         postal_code = replace(postal_code, district_delivery== "badia", "25132"),
         postal_code = replace(postal_code, district_delivery== "corona", "25079"),
         postal_code = replace(postal_code, district_delivery== "tovo", "25088"),
         postal_code = replace(postal_code, is.na(postal_code), "UNKNOWN"),
         postal_code = replace(postal_code, postal_code == "30016", "UNKNOWN"),
         postal_code = replace(postal_code, postal_code == "V9410", "UNKNOWN"))


data_exploratory <- filter(data_exploratory, data_exploratory$postal_code != "20020")
```

### "Administrative_area_level_3"

I manage "administrative_area_level_3" and the the missing values in there

```{r}
data_clean <- data_clean %>% select(-locality) %>%  rename("locality_delivery" = district_delivery, "district_delivery" = administrative_area_level_3)


# CODE NOT NEEDED SO FAR ---------------------------------------------
capunique <- data_clean %>% select(district_delivery, postal_code)   
capunique <- unique(capunique)
attempt <- left_join(data_clean, capunique, by = "postal_code")
# ----------------------------------------------------------------------

data_clean <- data_clean %>% 
              mutate_all(funs(tolower(.)))

```


2) I **turn the addresses into coordinates** with the google geocoding as well, creating a variable address_lat and variable address_long.

In this case it is necessary to reduce the variable address_distinct to its minimum, ensuring to have optimally fixed the differences of spelling,  since the google geocoding system allows to make 2.5k queries per day.

```{r}
# NOT STRICTLY NECESSARY SO FAR
```



# 5. WORKING WITH DATES 

_ Duplicate the variable "delivery_day" in order to keep it after I will have it split as following:

```{r}
data_clean <- data_clean %>% mutate(delivery_date = delivery_day)
```

_ Convert character string to Dates regarding the variables "dep_to_bs", "arr_to_bs", "delivery_day"
```{r}
data_clean <- data_clean %>% mutate(dep_to_bs = as.Date( x = dep_to_bs, format = "%d / %m / %y"),
                                    arr_to_bs = as.Date(x = arr_to_bs, format = "%d / %m / %y"),
                                    delivery_date = as.Date(x = delivery_date, format = "%d / %m / %y"))
```


_ Obtain the variable "weekday_deliv"
```{r}
data_clean <- data_clean %>% mutate(weekday_deliv = format(delivery_date, "%a"))  
```



_ Split delivery_day column into 3 variables
```{r}
data_clean <- data_clean %>% separate(delivery_day, c("day_deliv", "month_deliv", "year_deliv"), sep = "/")
```



_ Trasform dates in a better format  (ISSUE: IT TURNS THEM INTO CHARACTER AGAIN)
```{r}
data_clean <- data_clean %>% mutate(dep_to_bs = format( x = dep_to_bs, format = "%d/%m/%y"),
                                    arr_to_bs = format(x = arr_to_bs, format = "%d/%m/%y"))
```




_ In case I will just work with March data, I will not need variable "month_deliv" and "year_deliv"
```{r}
data_clean <- select(data_clean, - month_deliv, - year_deliv)
```



# 6. WORKING WITH HOURS
_ Convert character string to time regarding the variable "pickup_time"
(The following section code needs to be improved)
```{r}
data_clean <- data_clean %>% mutate(pickup_time =  as.POSIXct(x = data_clean$pickup_time, format = "%H:%M")) %>% 
              mutate(pickup_time = gsub(pattern = "2018-06-03", replacement = "",x = pickup_time))    
data_clean <- data_clean %>% mutate(pickup_time = times(x = pickup_time))

```

_ Create a variable to perform operation with hours, converting each hour in minutes (past midnight?)
```{r}
# HOW TO DO IT ?
```


# 7.CONVERT NECESSARY VARIABLES 

```{r}
data_clean <- data_clean %>% mutate(weight_pack = gsub(pattern = ",",replacement = ".", x = weight_pack))
data_clean$weight_pack <- as.double(data_clean$weight_pack)
class(data_clean$weight_pack)

data_clean$num_pack <- as.numeric(data_clean$num_pack)
data_clean$weekday_deliv <- factor(x = data_clean$weekday_deliv, levels = c("lun","mar","mer","gio","ven","sab"))
```

# 8.DELETE UNNECESSARY DRIVER, after a confrontation with the client

```{r}
data_clean <- data_clean %>% filter(driver_code != "208" & driver_code != "234"& driver_code != "260" & driver_code != "336" & driver_code != "404" & driver_code != "534" & driver_code != "535" & driver_code != "623"  & driver_code != "132")
```



# SUMMARY: CREATE NEW DATAFRAMES USEFUL FOR MY ANALYSIS

```{r}

data_exploratory <-  data_clean %>% select(-dep_to_bs, - arr_to_bs, month_deliv, - year_deliv, - sender_who, - pickup_who)

# TO BE COMPLETED DURING THE EXPLORATORY ANALYSIS
```


## --------------------------------------------------------------------------------


# CLEAN THE NEW DATASET OBTAINED FROM THE CLIENT

Each day of the month has been provided from the client in a different dataset, with the same variable.
By the end of this phase each day will be merged in a final dataset.

```{r}
library(readr)
Giornaliere_2018_03_01 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-01.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_02 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-02.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_05 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-05.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_06 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-06.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_07 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-07.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_08 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-08.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_09 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-09.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_12 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-12.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_13 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-13.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_14 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-14.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_15 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-15.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_16 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-16.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_19 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-19.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_20 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-20.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_21 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-21.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_22 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-22.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_23 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-23.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_26 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-26.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_27 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-27.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_28 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-28.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_29 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-29.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)
Giornaliere_2018_03_30 <- read_delim("~/Desktop/March tot/Giornaliere-2018-03-30.csv", 
                                     ";", escape_double = FALSE, locale = locale(encoding = "ISO-8859-1"), 
                                     trim_ws = TRUE)



# ERASING UNUSEFUL ROWS

day1 <- Giornaliere_2018_03_01[-c(1:5),]
day2 <- Giornaliere_2018_03_02[-c(1:5),]
day5 <- Giornaliere_2018_03_05[-c(1:5),]
day6 <- Giornaliere_2018_03_06[-c(1:5),]
day7 <- Giornaliere_2018_03_07[-c(1:5),]
day8 <- Giornaliere_2018_03_08[-c(1:5),]
day9 <- Giornaliere_2018_03_09[-c(1:5),]
day12 <- Giornaliere_2018_03_12[-c(1:5),]
day13 <- Giornaliere_2018_03_13[-c(1:5),]
day14 <- Giornaliere_2018_03_14[-c(1:5),]
day15<- Giornaliere_2018_03_15[-c(1:5),]
day16 <- Giornaliere_2018_03_16[-c(1:5),]
day19 <- Giornaliere_2018_03_19[-c(1:5),]
day20 <- Giornaliere_2018_03_20[-c(1:5),]
day21<- Giornaliere_2018_03_21[-c(1:5),]
day22<- Giornaliere_2018_03_22[-c(1:5),]
day23 <- Giornaliere_2018_03_23[-c(1:5),]
day26 <- Giornaliere_2018_03_26[-c(1:5),]
day27<- Giornaliere_2018_03_27[-c(1:5),]
day28<- Giornaliere_2018_03_28[-c(1:5),]
day29<- Giornaliere_2018_03_29[-c(1:5),]
day30 <- Giornaliere_2018_03_30[-c(1:5),]


# ADDING A NEW VARIABLE THAT WILL HELP TO IDENTIFY THE DAY ONCE THE DATAFRAME WILL BE MERGED TOGETHER

day1$day <- "1"
day2$day <- "2"
day5$day <- "5"
day6$day <- "6"
day7$day <- "7"
day8$day <- "8"
day9$day <- "9"
day12$day <- "12"
day13$day <- "13"
day14$day <- "14"
day15$day <- "15"
day16$day <- "16"
day19$day <- "19"
day20$day <- "20"
day21$day <- "21"
day22$day <- "22"
day23$day <- "23"
day26$day <- "26"
day27$day <- "27"
day28$day <- "28"
day29$day <- "29"
day30$day <- "30"


# MERGE THE DATA TOGETHER

library(dplyr)

new_datagls <- bind_rows(day1,day2,day5,day6,day7,day8,day9,day12,day13,day14,day15,day16,day19,day20,day21,day22,day23,day26,day27,day28,day29,day30)

new_datagls <- new_datagls %>% select(-simulazione,-o,-zona,-nolo)

# - - - - - - - - - - - - - - - - - - --  


library(tidyr)

new_datagls <- new_datagls %>% mutate(autista = sub(pattern = "\\s", replacement = "-", x = autista))
new_datagls <- new_datagls %>% separate(autista, c("driver_code", "driver_name"), sep = "-")


data1 <- data_clean %>% select(driver_code)
data1 <- unique(data1)
data3 <- setdiff(data2, data1)

newdatagls_noRT <- new_datagls %>% filter(new_datagls$sedi != "RT")   # Most of the driver codes that I did not need were from another factory


newdatagls_noRT <- newdatagls_noRT %>% rename("pack_loaded" = consegne)
RitiriNew <- RitiriNew  %>% filter(zrn_rit_prov == "BS")


aggregate_data4 <- newdatagls_noRT 
aggregate_data4 <- aggregate_data4 %>% rename("not_delivered" = rientri,
                                              "kg_delivered" = kg_cons,
                                              "pack_arrived" = arrivi,
                                              "pickup_services" = ritiri,
                                              "pickup_packs" = spd_rit )

save(aggregate_data4, file = "aggregate_data4.RData")
```

